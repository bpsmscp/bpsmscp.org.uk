---
title: "Web scraping Wikipedia data tables into R data frames"
author: "Mark Andrews"
date: '2019-01-28'
slug: scraping_wikipedia
stub: no
tags:
- web_scraping
- R
description: Wikipedia provides a lot of very useful tables of data. The data, however,
  are in the form of html tables, rather than some easy to import format like csv.
  To get this table into, for example, an R data frame requires some web scraping
  followed by data wrangling.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# We'll use these to style the tables, but we'll keep 
# it quiet.
library(knitr)
library(kableExtra)

stylish_Df <- function(Df){
  Df %>% 
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"),
                font_size = 10,
                full_width = TRUE)
}


```

[Wikipedia](https://en.wikipedia.org/) 
provides us with very large numbers of data tables. For example, here are
just five of the presumably tens, or maybe hundreds, of thousands of Wikipedia pages with data tables:

* [List of countries by GDP (nominal)](https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal))
* [List of highest-grossing Indian films](https://en.wikipedia.org/wiki/List_of_highest-grossing_Indian_films)
* [List of tallest buildings and structures](https://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures)
* [List of countries by intentional homicide rate](https://en.wikipedia.org/wiki/List_of_countries_by_intentional_homicide_rate)
* [List of countries and dependencies by area](https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_area)

All of these tables, perhaps with some exceptions, provide a detailed reference to the original sources of the data and, by virtue of being on a Wikipedia page, are free to use according to 
the [Creative Commons BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/) licence.
However, if we want to use data analysis using R on data in these tables, 
we must first do some web scraping
to extract the data from the html tables 
and then use `dplyr` and friends to clean them up
and convert them into a form that is easy to work with. Here, we provide a worked 
example of how to do this. 

For this example, the page we will work with is [List of English districts by population](https://en.wikipedia.org/wiki/List_of_English_districts_by_population), 
and in particular the [page version](https://en.wikipedia.org/w/index.php?title=List_of_English_districts_by_population&oldid=879013307) as of 28 January 2019 10:38 UTC. Here's a screenshot of that page where we see the main table:
```{r, out.width = "800px", echo=F, fig.align='center'}
knitr::include_graphics("img/english_districts_wikipage_screenshot.png")
```

We'll need some R packages for web scraping, namely [rvest](https://cran.r-project.org/web/packages/rvest/),
which provides a set of functions that are wrappers around functions in the [`httr`](https://cran.r-project.org/web/packages/httr) and [`xml2`](https://cran.r-project.org/web/packages/xml2/) packages.
We'll then need [`dplyr`](https://cran.r-project.org/web/packages/dplyr/)
for re-formatting and cleaning the data. We'll use [`magrittr`](https://cran.r-project.org/web/packages/magrittr/)
for some processing pipelines. We'll use [`stringr`](https://cran.r-project.org/web/packages/stringr/)
for some renaming. We'll use [`ggplot2`]() at the end for some
visualizations.

```{r}
library(rvest)
library(dplyr)
library(magrittr)
library(stringr)
library(ggplot2)
```

## Web scraping the table

The web scraping is relatively painless. In addtion to the url of the webpage, 
we also need to know the [xpath](https://en.wikipedia.org/wiki/XPath) node of the table in the page. 
This can be obtained using inspector tools in modern web-browsers, such as those provided by the DevTools in Chrome,
see [here](https://developers.google.com/web/tools/chrome-devtools/inspect-styles/).
On the webpage used in this example, there's actually just one html table, and
we convert that to a [tibble](https://r4ds.had.co.nz/tibbles.html), although that final step is not strictly necessary.

```{r}
webpage_url <- 'https://en.wikipedia.org/w/index.php?title=List_of_English_districts_by_population&oldid=879013307'

webpage_tables <- 
  webpage_url %>% 
  read_html() %>% 
  html_nodes(xpath='//*[@id="mw-content-text"]/div/table') %>% 
  html_table()

Df <- webpage_tables[[1]] %>%   # There's only one table
  as_tibble()
```


## Data wrangling 

Having read in the data into a data-frame, we now must 
do a series of typical `dplyr` data wrangling steps. 

First, let's take a look at the first 10 rows of the not-so-pretty `Df` that we've obtained:
```{r, echo=F}
Df %>% head(10) %>% stylish_Df()
```

There are a few things that need to be fixed:

* First, though this is maybe just a personal preference, we'll convert column names to 
  lower cases and containing no spaces.
* The original table's sub-headers like `More than 1,000,000 inhabitants`, `More than 500,000 inhabitants`, etc., have turned into rows with repeated values, and these need to be removed.
* The population values need to be converted to integers.
* Some variables can be dropped to make a cleaner data frame.


To convert the column names, I'll create a function using `stringr` and apply it using `rename_all`:
```{r}
tolower_no_spaces <- function(s){
  s %>% 
    str_to_lower() %>% 
    str_replace_all(' ', '_')
}

Df %<>% rename_all(tolower_no_spaces)  
  
```


Now, let's take a look again at the first 10 rows:
```{r, echo=F}
Df %>% head(10) %>% stylish_Df()
```


To deal with the inclusion of sub-headers rows,
we need to filter out all rows that contain values like 
`250,000 to 300,000 inhabitants`,
`Below 50,000`, etc. If we just look at the `rank` variable, these
cases can be relatively easily identified by whether they contain the words `to`, `than` or `Below`.

```{r, eval=F}
Df %>% filter(str_detect(rank, 'to|than|Below'))
```


```{r, echo=F}
Df %>% filter(str_detect(rank, 'to|than|Below')) %>% stylish_Df()
```

That gives us an easy rule with which to filter them out:
```{r}
Df %<>% filter(!str_detect(rank, 'to|than|Below')) 
```

We can now verify that the number of rows that we now have is the correct number, namely 326 (which is the current number of official [districts](https://en.wikipedia.org/wiki/Districts_of_England) that are in England).
```{r}
nrow(Df)
```


Now, let's take a look again at the first 10 rows:
```{r, echo=F}
Df %>% head(10) %>% stylish_Df()
```

Next, we'll convert the `population` variable to an 
`integer` variable by first removing the
commas in their values. Then, we'll drop `rank` entirely 
because it can be calculated easily from `population`.

```{r}
Df %<>% mutate(population = str_replace_all(population, ',', '') %>% as.integer()) %>% 
  select(-rank)
```

Let's look again at the first 10 rows:
```{r, echo=F}
Df %>% head(10) %>% stylish_Df()
```

The `type` variable is quite inconsistent. The 326 English districts are 
in fact divided into 5 types as follows: *metropolitan boroughs* (36), *London boroughs* (32), *non-metropolitan districts* (201), *unitary authorities* (55), and two *sui generis* districts, namely the City of London and the Isles of Scilly. In our `Df`, the type variable 
is missing is many cases. In other cases, the value listed, e.g. "City (2012)", 
does not signify one the five type just mentioned. In some other cases, 
it has two pieces of information that are separated by a comma.
The first is English district type, provides us with other information, such as 
whether the district officially has a [UK City status](https://en.wikipedia.org/wiki/City_status_in_the_United_Kingdom).
Because of this overall inconsistency, we will drop the variable. However, it
is possible to recover this information from joining this table with other tables of English districts
such as from what is available on the [List of English districts by area](https://en.wikipedia.org/wiki/List_of_English_districts_by_area) Wikipedia page. 

```{r}
Df %<>% select(-type) 
```

Looking again at the first 10 rows:
```{r, echo=F}
Df %>% head(10) %>% stylish_Df
```

As a final fix, we can see that one district, namely *Stockton-on-Tees*, is listed as belonging
to two regions of England:
```{r, eval=F}
Df %>% filter(district == "Stockton-on-Tees")
```
```{r, echo=F}
Df %>% filter(district == "Stockton-on-Tees") %>% 
  stylish_Df()
```
Given that this district is listed [here](https://en.wikipedia.org/wiki/Borough_of_Stockton-on-Tees) 
as part of 
[North East England](https://en.wikipedia.org/wiki/North_East_England), 
we'll change its listed region to `North East`.
```{r}
Df %<>% mutate(english_region = recode(english_region, 
                                       'North East andYorkshire and the Humber' = 'North East')
)
```


## Summarizing and visualizing the data

Having extracted and cleaned the data, we can now obtain some 
summary statistics:
```{r, eval=F}
Df %>% summarize(number_of_districts = n(),
                 median_population = median(population),
                 population_iqr = IQR(population)
)
```
```{r, echo=F}
Df %>% summarize(number_of_districts = n(),
                 median_population = median(population),
                 population_iqr = IQR(population)
) %>% stylish_Df()
```

And we can provide these summaries by English region too. 
```{r, eval=F}
Df %>% group_by(english_region) %>% 
  summarize(number_of_districts = n(),
            median_population = median(population),
            population_iqr = IQR(population)
  )
```
```{r, echo=F}
Df %>% group_by(english_region) %>% 
  summarize(number_of_districts = n(),
            median_population = median(population),
            population_iqr = IQR(population)
  ) %>% stylish_Df()
```


We can visualize the distributions of the population in each district.
```{r, fig.align="center"}
ggplot(Df,
       aes(x = population)
) + geom_histogram(bins = 25, col='white') + 
  theme_classic() +
  scale_x_continuous(name="Population", 
                     breaks = c(10^5, 5*10^5, 10^6),
                     labels = c('100K', '500K', '1M')
        )
```

And do the same by region.
```{r, fig.align="center"}
ggplot(Df,
       aes(x = population)
) + geom_histogram(bins = 25, col='white') + 
  facet_wrap(~english_region)+  theme_classic() +
 theme(strip.background = element_blank()) + scale_x_continuous(name="Population", 
                   breaks = c(10^5, 5*10^5, 10^6),
                   labels = c('100K', '500K', '1M')
       )
                   
                   
```

